{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from config import Config\n",
    "from datetime import datetime\n",
    "from img_embedding import ImageEmbedding\n",
    "from img_transformer import ImgTransformer\n",
    "from img_util import show_img_tensor_CHW\n",
    "from fliker_comment_tokenizer import FlikerCommentTokenizer\n",
    "from fliker_img_comment_dataset import ImgCommentDataset\n",
    "from model_util import count_parameters\n",
    "from pathlib import Path\n",
    "from text_token_embedding import TextTokenEmbedding\n",
    "from text_casual_mask_transformer import TextMaskedTransformer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as VF\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched img id: /tmp/enriched_results.csv\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "Enriched img id: /tmp/enriched_results.csv\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "Enriched img id: /tmp/enriched_results.csv\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "train_dataset:  114418\n",
      "eval_dataset:  28605\n",
      "test_dataset:  15892\n",
      "train_dataloader:  5721\n",
      "eval_data_loader:  1431\n",
      "test_data_loader:  795\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "train_dataset = ImgCommentDataset(config, split=\"train\")\n",
    "eval_dataset = ImgCommentDataset(config, split=\"eval\")\n",
    "test_dataset= ImgCommentDataset(config, split=\"test\")\n",
    "print(f\"train_dataset:  {len(train_dataset)}\")\n",
    "print(f\"eval_dataset:  {len(eval_dataset)}\")\n",
    "print(f\"test_dataset:  {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "# Data Loader\n",
    "BATCH_SIZE = 20\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"train_dataloader:  {len(train_dataloader)}\")\n",
    "print(f\"eval_data_loader:  {len(eval_dataloader)}\")\n",
    "print(f\"test_data_loader:  {len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgCaptionModel(nn.Module):\n",
    "    def __init__(self, config: Config, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config is not None\n",
    "        assert config.img_patch_embedding == config.text_token_embedding, f\"img_patch_embedding: {config.img_patch_embedding} should be same as text_token_embedding: {config.text_token_embedding}\"\n",
    "\n",
    "        self.config = config        \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # [<imge> x IMG_PATCHES][<bos>][TEXT_TOKEN_EMB x N][<pad>*]\n",
    "        # ---------------------------------------------------------\n",
    "        #           i0  i1  b   t1  t2  t3\n",
    "        # <image> | 1,  1,  1,  1,  1,  1 |\n",
    "        # <image> | 1,  1,  1,  1,  1,  1 | \n",
    "        # <bos>   | 1,  1,  1,  1,  1,  1 |\n",
    "        # t1      | 1,  1,  1,  1,  0,  0 |\n",
    "        # t2      | 1,  1,  1,  1,  1,  0 |\n",
    "        # t3      | 1,  1,  1,  1,  1,  1 |\n",
    "        self.img_bos_mask = torch.ones(size=(config.img_patches + 1, config.img_patches + 1 + config.max_text_len)) # (IMG_PATCHES + 1) x (IMG_PATCHES + 1 + TEXT_TOKENS)\n",
    "        self.text_mask = torch.hstack([\n",
    "            torch.ones(size=(config.max_text_len, config.img_patches + 1)),\n",
    "            torch.tril(torch.ones(size=(config.max_text_len, config.max_text_len)))\n",
    "        ])\n",
    "        self.mask = torch.vstack([self.img_bos_mask, self.text_mask])\n",
    "        self.transformer = TextMaskedTransformer(config=config, mask=self.mask)\n",
    "        self.lm_head = nn.Linear(config.text_token_embedding, tokenizer.vocab_size, bias=False)\n",
    "        # B x tokens x token_emb @ token_emb x vocab_size => B x tokens x vocab_size\n",
    "\n",
    "    def forward(self, img_feature: torch.tensor, text_feature: torch.tensor, text_mask: torch.tensor, target_text_token: torch.tensor, bos_embedding: torch.tensor): \n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            - img_feature: B x IMG_PATCHES x IMG_PATCH_EMB\n",
    "            - text_feature: B x TEXT_TOKEN x TEXT_EMB\n",
    "            - text_mask: B x TEXT_TOKEN x 1\n",
    "            - target_text_token: B x TEXT_TOKEN\n",
    "        outputs:\n",
    "            - text prediction: \n",
    "            - loss\n",
    "        \"\"\"\n",
    "        bos_embedding = bos_embedding.view(1, 1, -1) # 1 x 1 x TEXT_EMB\n",
    "        bos_embedding = bos_embedding.to(img_feature.device)\n",
    "        assert len(img_feature.size()) == len(bos_embedding.size()) == len(text_feature.size())\n",
    "        bos_embedding_ext = bos_embedding.expand(img_feature.size()[0], -1, -1)\n",
    "        x = torch.cat([img_feature, bos_embedding_ext, text_feature], dim=1) # B x [IMG_PATCHES + 1 + TEXT_TOKEN] x IMG_PATCH_EMB\n",
    "        x = self.transformer(x=x, need_embedding=False) # B x [IMG_PATCHES + 1 + TEXT_TOKEN] x IMG_PATCH_EMB\n",
    "        x =  self.lm_head(x) # B x [IMG_PATCHES + 1 + TEXT_TOKEN] x vocab_size\n",
    "\n",
    "        if target_text_token is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # extract the last `self.config.max_text_len` token positions\n",
    "            text_pos_mask = torch.arange(start=-50, end=0, step=1)\n",
    "            text_logits = x[:, text_pos_mask, :] # B x TEXT_TOKEN x vocab_size\n",
    "            B, TEXT_TOKEN, vocab_size = text_logits.size()\n",
    "            text_logits = text_logits.view(B*TEXT_TOKEN, -1)\n",
    "            target_text_token = target_text_token.view(B*TEXT_TOKEN)\n",
    "            loss = F.cross_entropy(text_logits, target_text_token)\n",
    "\n",
    "        return text_logits, loss\n",
    "\n",
    "\n",
    "class ImgLanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.img_embedding = ImageEmbedding(config=config)\n",
    "        self.img_transfomer = ImgTransformer(config=config)\n",
    "        self.img_flatten = nn.Flatten(start_dim=1)\n",
    "        self.img_proj = nn.Linear(in_features=config.img_patches* config.img_patch_embedding, out_features=config.img_text_proj_features)\n",
    "        self.img_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # self.text_embedding = TextTokenEmbedding(config=config)\n",
    "        self.text_transformer = TextMaskedTransformer(config=config)\n",
    "        self.text_flatten = nn.Flatten(start_dim=1)\n",
    "        self.text_proj = nn.Linear(in_features=config.max_text_len* config.text_token_embedding, out_features=config.img_text_proj_features)\n",
    "        self.text_softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.diag_mask = torch.diag(torch.ones(config.img_text_proj_features))\n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "\n",
    "        self.bos_token = self.text_transformer.text_token_embedding.text_encoder.encode(\"<bos>\")[1]  # return <bos><bos>\n",
    "        self.img_caption_model = ImgCaptionModel(config=config, tokenizer=self.text_transformer.text_token_embedding.text_encoder)\n",
    "\n",
    "    def forward(self, batch_img_tensor: torch.tensor, batch_text_tensor: torch.tensor, batch_img_id_tensor: torch.tensor=None):\n",
    "        \"\"\"\n",
    "        batch_img_tensor: B x IMG_PATCHES x IMG_EMB\n",
    "        batch_text_tensor: B x TEXT_TOKEN\n",
    "        \"\"\"\n",
    "        img_embedding = self.img_embedding(batch_img_tensor) # B x IMG_PATCHES x IMG_EMB\n",
    "        # print(f\"img_encoding: {img_embedding.size()}\")\n",
    "\n",
    "        img_feature = self.img_transfomer(img_embedding) # B x IMG_PATCHES x IMG_EMB\n",
    "        # print(f\"img_feature: {img_feature.size()}\")\n",
    "\n",
    "        img_feature_flatten = self.img_flatten(img_feature)\n",
    "        # print(f\"img_feature_flatten: {img_feature_flatten.size()}\")\n",
    "\n",
    "        img_feature_proj = self.img_proj(img_feature_flatten)\n",
    "        # print(f\"img_feature_proj: {img_feature_proj.size()}\")  # B x img_text_proj_features\n",
    "\n",
    "        # text_embedding = self.text_embedding(batch_text_tensor)\n",
    "        # print(f\"text_embedding: {text_embedding.size()}\")\n",
    "\n",
    "        text_feature = self.text_transformer(batch_text_tensor)\n",
    "        # print(f\"text_feature: {text_feature.size()}\")\n",
    "\n",
    "        text_feature_flatten = self.text_flatten(text_feature)\n",
    "        # print(f\"text_feature_flatten: {text_feature_flatten.size()}\")\n",
    "\n",
    "        text_feature_proj = self.text_proj(text_feature_flatten)\n",
    "        # print(f\"text_feature_proj: {text_feature_proj.size()}\")  # B x img_text_proj_features\n",
    "\n",
    "        # Contrastive learning\n",
    "        contrastive_scores = img_feature_proj @ text_feature_proj.T\n",
    "        # print(f\"contractive_scores: {contrastive_scores}\")  # B x img_text_proj_features\n",
    "\n",
    "        img_contrastive_prob = self.img_softmax(contrastive_scores)\n",
    "        # print(f\"img_contrastive_prob: {img_contrastive_prob}\")  # B x img_text_proj_features\n",
    "        \n",
    "        target = torch.arange(img_contrastive_prob.size()[0], device=img_contrastive_prob.device)\n",
    "        img_loss = self.loss_fn(img_contrastive_prob, target) / img_contrastive_prob.size()[0]\n",
    "        # img_loss = self.loss_fn(img_contrastive_prob, self.target.expand(img_contrastive_prob.size()[0], -1))\n",
    "        # print(f\"img_loss: {img_loss}\")\n",
    "\n",
    "        text_contrastive_prob = self.text_softmax(contrastive_scores.T)\n",
    "        # print(f\"text_contrastive_prob: {text_contrastive_prob}\")  # B x img_text_proj_features\n",
    "        text_loss = self.loss_fn(text_contrastive_prob, target)  / text_contrastive_prob.size()[0]\n",
    "        # print(f\"text_loss: {text_loss}\")\n",
    "\n",
    "        bos_embedding = self.text_transformer.text_token_embedding(torch.tensor(self.bos_token, device=batch_img_tensor.device)) \n",
    "        lm_logits, lm_loss = self.img_caption_model(img_feature=img_feature, text_feature=text_feature, text_mask=(batch_text_tensor!=0), target_text_token=batch_text_tensor, bos_embedding=bos_embedding)\n",
    "        # print(f\"lm_logits: {lm_logits.size()}\")\n",
    "        # print(f\"lm_loss: {lm_loss}\")\n",
    "        \n",
    "        return img_loss, text_loss, img_contrastive_prob, text_contrastive_prob, lm_loss, lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_img_tensor: torch.Size([20, 3, 224, 224])\n",
      "batch_img_id_tensor: torch.Size([20])\n",
      "batch_comment_encoding: torch.Size([20, 50])\n",
      "batch_comment_mask: torch.Size([20, 50])\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = ImgCommentDataset(config, split=\"train\")\n",
    "\n",
    "# BATCH_SIZE = 10\n",
    "# train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "batch_img_tensor, batch_img_id_tensor, batch_comment_encoding, batch_comment_mask = next(iter(train_dataloader))\n",
    "print(f\"batch_img_tensor: {batch_img_tensor.size()}\")\n",
    "print(f\"batch_img_id_tensor: {batch_img_id_tensor.size()}\")\n",
    "print(f\"batch_comment_encoding: {batch_comment_encoding.size()}\")\n",
    "print(f\"batch_comment_mask: {batch_comment_mask.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "img_loss: 0.36636996269226074\n",
      "text_loss: 0.257795125246048\n",
      "lm_loss: 11.646864891052246\n",
      "pytorch_total_params: 568.73676 m\n",
      "pytorch_total_trainable_params: 568.73676 m\n",
      "+------------------------------------------------------------------------+------------+\n",
      "|                                Modules                                 | Parameters |\n",
      "+------------------------------------------------------------------------+------------+\n",
      "|                       img_embedding.conv.weight                        |   559104   |\n",
      "|                        img_embedding.conv.bias                         |    728     |\n",
      "|                   img_embedding.pos_embedding.weight                   |   142688   |\n",
      "|         img_transfomer.blocks.0.multihead_attention.wq.weight          |   529984   |\n",
      "|          img_transfomer.blocks.0.multihead_attention.wq.bias           |    728     |\n",
      "|         img_transfomer.blocks.0.multihead_attention.wk.weight          |   529984   |\n",
      "|          img_transfomer.blocks.0.multihead_attention.wk.bias           |    728     |\n",
      "|         img_transfomer.blocks.0.multihead_attention.wv.weight          |   529984   |\n",
      "|          img_transfomer.blocks.0.multihead_attention.wv.bias           |    728     |\n",
      "|        img_transfomer.blocks.0.multihead_attention.norm.weight         |    728     |\n",
      "|         img_transfomer.blocks.0.multihead_attention.norm.bias          |    728     |\n",
      "|                  img_transfomer.blocks.0.norm.weight                   |    728     |\n",
      "|                   img_transfomer.blocks.0.norm.bias                    |    728     |\n",
      "|                  img_transfomer.blocks.0.mlp.0.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.0.mlp.0.bias                   |    2912    |\n",
      "|                  img_transfomer.blocks.0.mlp.2.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.0.mlp.2.bias                   |    728     |\n",
      "|         img_transfomer.blocks.1.multihead_attention.wq.weight          |   529984   |\n",
      "|          img_transfomer.blocks.1.multihead_attention.wq.bias           |    728     |\n",
      "|         img_transfomer.blocks.1.multihead_attention.wk.weight          |   529984   |\n",
      "|          img_transfomer.blocks.1.multihead_attention.wk.bias           |    728     |\n",
      "|         img_transfomer.blocks.1.multihead_attention.wv.weight          |   529984   |\n",
      "|          img_transfomer.blocks.1.multihead_attention.wv.bias           |    728     |\n",
      "|        img_transfomer.blocks.1.multihead_attention.norm.weight         |    728     |\n",
      "|         img_transfomer.blocks.1.multihead_attention.norm.bias          |    728     |\n",
      "|                  img_transfomer.blocks.1.norm.weight                   |    728     |\n",
      "|                   img_transfomer.blocks.1.norm.bias                    |    728     |\n",
      "|                  img_transfomer.blocks.1.mlp.0.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.1.mlp.0.bias                   |    2912    |\n",
      "|                  img_transfomer.blocks.1.mlp.2.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.1.mlp.2.bias                   |    728     |\n",
      "|         img_transfomer.blocks.2.multihead_attention.wq.weight          |   529984   |\n",
      "|          img_transfomer.blocks.2.multihead_attention.wq.bias           |    728     |\n",
      "|         img_transfomer.blocks.2.multihead_attention.wk.weight          |   529984   |\n",
      "|          img_transfomer.blocks.2.multihead_attention.wk.bias           |    728     |\n",
      "|         img_transfomer.blocks.2.multihead_attention.wv.weight          |   529984   |\n",
      "|          img_transfomer.blocks.2.multihead_attention.wv.bias           |    728     |\n",
      "|        img_transfomer.blocks.2.multihead_attention.norm.weight         |    728     |\n",
      "|         img_transfomer.blocks.2.multihead_attention.norm.bias          |    728     |\n",
      "|                  img_transfomer.blocks.2.norm.weight                   |    728     |\n",
      "|                   img_transfomer.blocks.2.norm.bias                    |    728     |\n",
      "|                  img_transfomer.blocks.2.mlp.0.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.2.mlp.0.bias                   |    2912    |\n",
      "|                  img_transfomer.blocks.2.mlp.2.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.2.mlp.2.bias                   |    728     |\n",
      "|         img_transfomer.blocks.3.multihead_attention.wq.weight          |   529984   |\n",
      "|          img_transfomer.blocks.3.multihead_attention.wq.bias           |    728     |\n",
      "|         img_transfomer.blocks.3.multihead_attention.wk.weight          |   529984   |\n",
      "|          img_transfomer.blocks.3.multihead_attention.wk.bias           |    728     |\n",
      "|         img_transfomer.blocks.3.multihead_attention.wv.weight          |   529984   |\n",
      "|          img_transfomer.blocks.3.multihead_attention.wv.bias           |    728     |\n",
      "|        img_transfomer.blocks.3.multihead_attention.norm.weight         |    728     |\n",
      "|         img_transfomer.blocks.3.multihead_attention.norm.bias          |    728     |\n",
      "|                  img_transfomer.blocks.3.norm.weight                   |    728     |\n",
      "|                   img_transfomer.blocks.3.norm.bias                    |    728     |\n",
      "|                  img_transfomer.blocks.3.mlp.0.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.3.mlp.0.bias                   |    2912    |\n",
      "|                  img_transfomer.blocks.3.mlp.2.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.3.mlp.2.bias                   |    728     |\n",
      "|         img_transfomer.blocks.4.multihead_attention.wq.weight          |   529984   |\n",
      "|          img_transfomer.blocks.4.multihead_attention.wq.bias           |    728     |\n",
      "|         img_transfomer.blocks.4.multihead_attention.wk.weight          |   529984   |\n",
      "|          img_transfomer.blocks.4.multihead_attention.wk.bias           |    728     |\n",
      "|         img_transfomer.blocks.4.multihead_attention.wv.weight          |   529984   |\n",
      "|          img_transfomer.blocks.4.multihead_attention.wv.bias           |    728     |\n",
      "|        img_transfomer.blocks.4.multihead_attention.norm.weight         |    728     |\n",
      "|         img_transfomer.blocks.4.multihead_attention.norm.bias          |    728     |\n",
      "|                  img_transfomer.blocks.4.norm.weight                   |    728     |\n",
      "|                   img_transfomer.blocks.4.norm.bias                    |    728     |\n",
      "|                  img_transfomer.blocks.4.mlp.0.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.4.mlp.0.bias                   |    2912    |\n",
      "|                  img_transfomer.blocks.4.mlp.2.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.4.mlp.2.bias                   |    728     |\n",
      "|         img_transfomer.blocks.5.multihead_attention.wq.weight          |   529984   |\n",
      "|          img_transfomer.blocks.5.multihead_attention.wq.bias           |    728     |\n",
      "|         img_transfomer.blocks.5.multihead_attention.wk.weight          |   529984   |\n",
      "|          img_transfomer.blocks.5.multihead_attention.wk.bias           |    728     |\n",
      "|         img_transfomer.blocks.5.multihead_attention.wv.weight          |   529984   |\n",
      "|          img_transfomer.blocks.5.multihead_attention.wv.bias           |    728     |\n",
      "|        img_transfomer.blocks.5.multihead_attention.norm.weight         |    728     |\n",
      "|         img_transfomer.blocks.5.multihead_attention.norm.bias          |    728     |\n",
      "|                  img_transfomer.blocks.5.norm.weight                   |    728     |\n",
      "|                   img_transfomer.blocks.5.norm.bias                    |    728     |\n",
      "|                  img_transfomer.blocks.5.mlp.0.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.5.mlp.0.bias                   |    2912    |\n",
      "|                  img_transfomer.blocks.5.mlp.2.weight                  |  2119936   |\n",
      "|                   img_transfomer.blocks.5.mlp.2.bias                   |    728     |\n",
      "|                            img_proj.weight                             | 146112512  |\n",
      "|                             img_proj.bias                              |    1024    |\n",
      "|        text_transformer.text_token_embedding.embeddings.weight         |  93184000  |\n",
      "|        text_transformer.blocks.0.multihead_attention.wq.weight         |   529984   |\n",
      "|         text_transformer.blocks.0.multihead_attention.wq.bias          |    728     |\n",
      "|        text_transformer.blocks.0.multihead_attention.wk.weight         |   529984   |\n",
      "|         text_transformer.blocks.0.multihead_attention.wk.bias          |    728     |\n",
      "|        text_transformer.blocks.0.multihead_attention.wv.weight         |   529984   |\n",
      "|         text_transformer.blocks.0.multihead_attention.wv.bias          |    728     |\n",
      "|       text_transformer.blocks.0.multihead_attention.norm.weight        |    728     |\n",
      "|        text_transformer.blocks.0.multihead_attention.norm.bias         |    728     |\n",
      "|                 text_transformer.blocks.0.norm.weight                  |    728     |\n",
      "|                  text_transformer.blocks.0.norm.bias                   |    728     |\n",
      "|                 text_transformer.blocks.0.mlp.0.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.0.mlp.0.bias                  |    2912    |\n",
      "|                 text_transformer.blocks.0.mlp.2.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.0.mlp.2.bias                  |    728     |\n",
      "|        text_transformer.blocks.1.multihead_attention.wq.weight         |   529984   |\n",
      "|         text_transformer.blocks.1.multihead_attention.wq.bias          |    728     |\n",
      "|        text_transformer.blocks.1.multihead_attention.wk.weight         |   529984   |\n",
      "|         text_transformer.blocks.1.multihead_attention.wk.bias          |    728     |\n",
      "|        text_transformer.blocks.1.multihead_attention.wv.weight         |   529984   |\n",
      "|         text_transformer.blocks.1.multihead_attention.wv.bias          |    728     |\n",
      "|       text_transformer.blocks.1.multihead_attention.norm.weight        |    728     |\n",
      "|        text_transformer.blocks.1.multihead_attention.norm.bias         |    728     |\n",
      "|                 text_transformer.blocks.1.norm.weight                  |    728     |\n",
      "|                  text_transformer.blocks.1.norm.bias                   |    728     |\n",
      "|                 text_transformer.blocks.1.mlp.0.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.1.mlp.0.bias                  |    2912    |\n",
      "|                 text_transformer.blocks.1.mlp.2.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.1.mlp.2.bias                  |    728     |\n",
      "|        text_transformer.blocks.2.multihead_attention.wq.weight         |   529984   |\n",
      "|         text_transformer.blocks.2.multihead_attention.wq.bias          |    728     |\n",
      "|        text_transformer.blocks.2.multihead_attention.wk.weight         |   529984   |\n",
      "|         text_transformer.blocks.2.multihead_attention.wk.bias          |    728     |\n",
      "|        text_transformer.blocks.2.multihead_attention.wv.weight         |   529984   |\n",
      "|         text_transformer.blocks.2.multihead_attention.wv.bias          |    728     |\n",
      "|       text_transformer.blocks.2.multihead_attention.norm.weight        |    728     |\n",
      "|        text_transformer.blocks.2.multihead_attention.norm.bias         |    728     |\n",
      "|                 text_transformer.blocks.2.norm.weight                  |    728     |\n",
      "|                  text_transformer.blocks.2.norm.bias                   |    728     |\n",
      "|                 text_transformer.blocks.2.mlp.0.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.2.mlp.0.bias                  |    2912    |\n",
      "|                 text_transformer.blocks.2.mlp.2.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.2.mlp.2.bias                  |    728     |\n",
      "|        text_transformer.blocks.3.multihead_attention.wq.weight         |   529984   |\n",
      "|         text_transformer.blocks.3.multihead_attention.wq.bias          |    728     |\n",
      "|        text_transformer.blocks.3.multihead_attention.wk.weight         |   529984   |\n",
      "|         text_transformer.blocks.3.multihead_attention.wk.bias          |    728     |\n",
      "|        text_transformer.blocks.3.multihead_attention.wv.weight         |   529984   |\n",
      "|         text_transformer.blocks.3.multihead_attention.wv.bias          |    728     |\n",
      "|       text_transformer.blocks.3.multihead_attention.norm.weight        |    728     |\n",
      "|        text_transformer.blocks.3.multihead_attention.norm.bias         |    728     |\n",
      "|                 text_transformer.blocks.3.norm.weight                  |    728     |\n",
      "|                  text_transformer.blocks.3.norm.bias                   |    728     |\n",
      "|                 text_transformer.blocks.3.mlp.0.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.3.mlp.0.bias                  |    2912    |\n",
      "|                 text_transformer.blocks.3.mlp.2.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.3.mlp.2.bias                  |    728     |\n",
      "|        text_transformer.blocks.4.multihead_attention.wq.weight         |   529984   |\n",
      "|         text_transformer.blocks.4.multihead_attention.wq.bias          |    728     |\n",
      "|        text_transformer.blocks.4.multihead_attention.wk.weight         |   529984   |\n",
      "|         text_transformer.blocks.4.multihead_attention.wk.bias          |    728     |\n",
      "|        text_transformer.blocks.4.multihead_attention.wv.weight         |   529984   |\n",
      "|         text_transformer.blocks.4.multihead_attention.wv.bias          |    728     |\n",
      "|       text_transformer.blocks.4.multihead_attention.norm.weight        |    728     |\n",
      "|        text_transformer.blocks.4.multihead_attention.norm.bias         |    728     |\n",
      "|                 text_transformer.blocks.4.norm.weight                  |    728     |\n",
      "|                  text_transformer.blocks.4.norm.bias                   |    728     |\n",
      "|                 text_transformer.blocks.4.mlp.0.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.4.mlp.0.bias                  |    2912    |\n",
      "|                 text_transformer.blocks.4.mlp.2.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.4.mlp.2.bias                  |    728     |\n",
      "|        text_transformer.blocks.5.multihead_attention.wq.weight         |   529984   |\n",
      "|         text_transformer.blocks.5.multihead_attention.wq.bias          |    728     |\n",
      "|        text_transformer.blocks.5.multihead_attention.wk.weight         |   529984   |\n",
      "|         text_transformer.blocks.5.multihead_attention.wk.bias          |    728     |\n",
      "|        text_transformer.blocks.5.multihead_attention.wv.weight         |   529984   |\n",
      "|         text_transformer.blocks.5.multihead_attention.wv.bias          |    728     |\n",
      "|       text_transformer.blocks.5.multihead_attention.norm.weight        |    728     |\n",
      "|        text_transformer.blocks.5.multihead_attention.norm.bias         |    728     |\n",
      "|                 text_transformer.blocks.5.norm.weight                  |    728     |\n",
      "|                  text_transformer.blocks.5.norm.bias                   |    728     |\n",
      "|                 text_transformer.blocks.5.mlp.0.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.5.mlp.0.bias                  |    2912    |\n",
      "|                 text_transformer.blocks.5.mlp.2.weight                 |  2119936   |\n",
      "|                  text_transformer.blocks.5.mlp.2.bias                  |    728     |\n",
      "|                            text_proj.weight                            |  37273600  |\n",
      "|                             text_proj.bias                             |    1024    |\n",
      "|  img_caption_model.transformer.text_token_embedding.embeddings.weight  |  93184000  |\n",
      "|  img_caption_model.transformer.blocks.0.multihead_attention.wq.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.0.multihead_attention.wq.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.0.multihead_attention.wk.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.0.multihead_attention.wk.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.0.multihead_attention.wv.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.0.multihead_attention.wv.bias   |    728     |\n",
      "| img_caption_model.transformer.blocks.0.multihead_attention.norm.weight |    728     |\n",
      "|  img_caption_model.transformer.blocks.0.multihead_attention.norm.bias  |    728     |\n",
      "|           img_caption_model.transformer.blocks.0.norm.weight           |    728     |\n",
      "|            img_caption_model.transformer.blocks.0.norm.bias            |    728     |\n",
      "|          img_caption_model.transformer.blocks.0.mlp.0.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.0.mlp.0.bias            |    2912    |\n",
      "|          img_caption_model.transformer.blocks.0.mlp.2.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.0.mlp.2.bias            |    728     |\n",
      "|  img_caption_model.transformer.blocks.1.multihead_attention.wq.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.1.multihead_attention.wq.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.1.multihead_attention.wk.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.1.multihead_attention.wk.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.1.multihead_attention.wv.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.1.multihead_attention.wv.bias   |    728     |\n",
      "| img_caption_model.transformer.blocks.1.multihead_attention.norm.weight |    728     |\n",
      "|  img_caption_model.transformer.blocks.1.multihead_attention.norm.bias  |    728     |\n",
      "|           img_caption_model.transformer.blocks.1.norm.weight           |    728     |\n",
      "|            img_caption_model.transformer.blocks.1.norm.bias            |    728     |\n",
      "|          img_caption_model.transformer.blocks.1.mlp.0.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.1.mlp.0.bias            |    2912    |\n",
      "|          img_caption_model.transformer.blocks.1.mlp.2.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.1.mlp.2.bias            |    728     |\n",
      "|  img_caption_model.transformer.blocks.2.multihead_attention.wq.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.2.multihead_attention.wq.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.2.multihead_attention.wk.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.2.multihead_attention.wk.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.2.multihead_attention.wv.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.2.multihead_attention.wv.bias   |    728     |\n",
      "| img_caption_model.transformer.blocks.2.multihead_attention.norm.weight |    728     |\n",
      "|  img_caption_model.transformer.blocks.2.multihead_attention.norm.bias  |    728     |\n",
      "|           img_caption_model.transformer.blocks.2.norm.weight           |    728     |\n",
      "|            img_caption_model.transformer.blocks.2.norm.bias            |    728     |\n",
      "|          img_caption_model.transformer.blocks.2.mlp.0.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.2.mlp.0.bias            |    2912    |\n",
      "|          img_caption_model.transformer.blocks.2.mlp.2.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.2.mlp.2.bias            |    728     |\n",
      "|  img_caption_model.transformer.blocks.3.multihead_attention.wq.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.3.multihead_attention.wq.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.3.multihead_attention.wk.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.3.multihead_attention.wk.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.3.multihead_attention.wv.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.3.multihead_attention.wv.bias   |    728     |\n",
      "| img_caption_model.transformer.blocks.3.multihead_attention.norm.weight |    728     |\n",
      "|  img_caption_model.transformer.blocks.3.multihead_attention.norm.bias  |    728     |\n",
      "|           img_caption_model.transformer.blocks.3.norm.weight           |    728     |\n",
      "|            img_caption_model.transformer.blocks.3.norm.bias            |    728     |\n",
      "|          img_caption_model.transformer.blocks.3.mlp.0.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.3.mlp.0.bias            |    2912    |\n",
      "|          img_caption_model.transformer.blocks.3.mlp.2.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.3.mlp.2.bias            |    728     |\n",
      "|  img_caption_model.transformer.blocks.4.multihead_attention.wq.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.4.multihead_attention.wq.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.4.multihead_attention.wk.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.4.multihead_attention.wk.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.4.multihead_attention.wv.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.4.multihead_attention.wv.bias   |    728     |\n",
      "| img_caption_model.transformer.blocks.4.multihead_attention.norm.weight |    728     |\n",
      "|  img_caption_model.transformer.blocks.4.multihead_attention.norm.bias  |    728     |\n",
      "|           img_caption_model.transformer.blocks.4.norm.weight           |    728     |\n",
      "|            img_caption_model.transformer.blocks.4.norm.bias            |    728     |\n",
      "|          img_caption_model.transformer.blocks.4.mlp.0.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.4.mlp.0.bias            |    2912    |\n",
      "|          img_caption_model.transformer.blocks.4.mlp.2.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.4.mlp.2.bias            |    728     |\n",
      "|  img_caption_model.transformer.blocks.5.multihead_attention.wq.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.5.multihead_attention.wq.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.5.multihead_attention.wk.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.5.multihead_attention.wk.bias   |    728     |\n",
      "|  img_caption_model.transformer.blocks.5.multihead_attention.wv.weight  |   529984   |\n",
      "|   img_caption_model.transformer.blocks.5.multihead_attention.wv.bias   |    728     |\n",
      "| img_caption_model.transformer.blocks.5.multihead_attention.norm.weight |    728     |\n",
      "|  img_caption_model.transformer.blocks.5.multihead_attention.norm.bias  |    728     |\n",
      "|           img_caption_model.transformer.blocks.5.norm.weight           |    728     |\n",
      "|            img_caption_model.transformer.blocks.5.norm.bias            |    728     |\n",
      "|          img_caption_model.transformer.blocks.5.mlp.0.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.5.mlp.0.bias            |    2912    |\n",
      "|          img_caption_model.transformer.blocks.5.mlp.2.weight           |  2119936   |\n",
      "|           img_caption_model.transformer.blocks.5.mlp.2.bias            |    728     |\n",
      "|                    img_caption_model.lm_head.weight                    |  93184000  |\n",
      "+------------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 568736760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "568736760"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImgLanguageModel(config=config)\n",
    "img_loss, text_loss, img_contrastive_prob, text_contrastive_prob, lm_loss, lm_logit = model(batch_img_tensor=batch_img_tensor, batch_text_tensor=batch_comment_encoding, batch_img_id_tensor=batch_img_id_tensor)\n",
    "print(f\"img_loss: {img_loss}\")\n",
    "print(f\"text_loss: {text_loss}\")\n",
    "print(f\"lm_loss: {lm_loss}\")\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"pytorch_total_params: {pytorch_total_params/10**6} m\")\n",
    "print(f\"pytorch_total_trainable_params: {pytorch_total_trainable_params/10**6} m\")\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits = torch.Size([5, 10])\n",
      "target = torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6687, -2.6239], device='mps:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.rand(size=(5, 10))\n",
    "target = torch.arange(5)\n",
    "print(f\"logits = {logits.size()}\")\n",
    "print(f\"target = {target.size()}\")\n",
    "F.cross_entropy(logits, target)\n",
    "lm_logit.size()\n",
    "device = torch.device(\"mps\")\n",
    "class TM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.v = nn.Embedding(1, 2)\n",
    "\n",
    "tm = TM()\n",
    "tv = tm.v(torch.tensor(0))\n",
    "tv\n",
    "tm = tm.to(device)\n",
    "tv1 = tm.v(torch.tensor(0, device=device))\n",
    "tv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-08-30 17:39:14 67175:6895956 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "[W CPUAllocator.cpp:235] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "STAGE:2024-08-30 17:39:39 67175:6895956 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-08-30 17:39:39 67175:6895956 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "EPOCHES = 5\n",
    "EVAL_INTERVAL = 100\n",
    "EVAL_STEPS = 10\n",
    "lr = 0.001\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer =  torch.optim.AdamW(params=model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=EPOCHES)\n",
    "\n",
    "def eval(model: ImgLanguageModel, global_step : int, writer: SummaryWriter):\n",
    "    model.eval()\n",
    "\n",
    "    avg_eval_loss = None\n",
    "    eval_loss_std = None\n",
    "    with torch.no_grad():\n",
    "        eval_losses = []\n",
    "        for i, data in enumerate(eval_dataloader):\n",
    "            if i > EVAL_STEPS:\n",
    "                # It takes significant time to do one full eval.\n",
    "                break\n",
    "\n",
    "            batch_img_tensor, batch_img_id_tensor, batch_target_tensor, batch_target_mask = data\n",
    "            batch_img_tensor = batch_img_tensor.to(device)\n",
    "            batch_img_id_tensor = batch_img_id_tensor.to(device)\n",
    "            batch_target_tensor = batch_target_tensor.to(device)\n",
    "            batch_target_mask = batch_target_mask.to(device)\n",
    "            img_loss, text_loss, img_contrastive_prob, text_contrastive_prob, lm_loss, lm_logit = model(batch_img_tensor, batch_target_tensor)\n",
    "            writer.add_scalar(\"eval/Img Loss\", img_loss, global_step)\n",
    "            writer.add_scalar(\"eval/Text Loss\", text_loss, global_step)\n",
    "            writer.add_scalar(\"eval/LM Loss\", lm_loss, global_step)\n",
    "            eval_losses.append(img_loss + text_loss + lm_loss)\n",
    "        eval_losses = torch.tensor(eval_losses)\n",
    "        avg_eval_loss = eval_losses.mean()\n",
    "        eval_loss_std = eval_losses.std()\n",
    "        writer.add_scalar(\"eval/Loss\", avg_eval_loss, global_step)\n",
    "        writer.add_scalar(\"Loss/eval-std\", eval_loss_std, global_step)\n",
    "    model.train()\n",
    "    writer.flush()\n",
    "    return avg_eval_loss, eval_loss_std\n",
    "    \n",
    "\n",
    "\n",
    "def train(model: ImgLanguageModel, writer: SummaryWriter):\n",
    "    best_vloss = torch.tensor(1_000_000)\n",
    "    with torch.profiler.profile(\n",
    "            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler('./runs'),\n",
    "            activities=[torch.profiler.ProfilerActivity.CPU], \n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True\n",
    "    ) as prof:\n",
    "    # with torch.mps.profiler.profile(mode=\"interval\", wait_until_completed=False):\n",
    "        for epoch in range(EPOCHES): \n",
    "            for train_step, data in enumerate(train_dataloader):\n",
    "                global_step = epoch * len(train_dataloader) + train_step\n",
    "\n",
    "                # Profile\n",
    "                if global_step < 1 + 1 + 3:\n",
    "                    prof.step()\n",
    "\n",
    "                batch_img_tensor, batch_img_id_tensor, batch_target_tensor, batch_target_mask = data\n",
    "                batch_img_tensor = batch_img_tensor.to(device)\n",
    "                batch_img_id_tensor = batch_img_id_tensor.to(device)\n",
    "                batch_target_tensor = batch_target_tensor.to(device)\n",
    "                batch_target_mask = batch_target_mask.to(device)\n",
    "\n",
    "                # Viz Model\n",
    "                # if global_step == 0:\n",
    "                #     writer.add_graph(model, (batch_img_tensor, batch_target_tensor))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                img_loss, text_loss, img_contrastive_prob, text_contrastive_prob, lm_loss, lm_logit = model(batch_img_tensor, batch_target_tensor)\n",
    "                writer.add_scalar(\"train/Img Loss\", img_loss, global_step)\n",
    "                writer.add_scalar(\"train/Text Loss\", text_loss, global_step)\n",
    "                writer.add_scalar(\"train/LM Loss\", lm_loss, global_step)\n",
    "                writer.add_scalar(\"train/Loss\", img_loss+text_loss+lm_loss, global_step)\n",
    "                writer.add_scalar(\"Learning Rate\", scheduler.get_last_lr()[-1], global_step)\n",
    "                loss = img_loss + text_loss + lm_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                if train_step > 0 and train_step % EVAL_INTERVAL == 0:\n",
    "                    avg_vloss, _ = eval(model=model, global_step=global_step, writer=writer)\n",
    "                \n",
    "                    if avg_vloss is not None and avg_vloss < best_vloss:\n",
    "                        best_vloss = avg_vloss\n",
    "                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                        model_path = f\"vlm_caption_model_{epoch}_{timestamp}\"\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "with SummaryWriter(flush_secs=1) as writer:\n",
    "    train(model=model, writer=writer)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    model_path = f\"vlm_caption_model_{timestamp}_final\"\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_path = f\"vlm_contrastive_model_0_{timestamp}_final\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = ImgLanguageModel(config=config)\n",
    "model_trained.load_state_dict(torch.load(model_path))\n",
    "model_trained = model_trained.to(device)\n",
    "model_trained.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_img_tensor, batch_img_id_tensor, batch_target_tensor, batch_target_mask_tensor = next(iter(test_dataloader))\n",
    "batch_img_tensor = batch_img_tensor.to(device)\n",
    "batch_target_tensor = batch_target_tensor.to(device)\n",
    "batch_target_mask_tensor = batch_target_mask_tensor.to(device)\n",
    "img_loss, text_loss, img_contrastive_prob, text_contrastive_prob, lm_loss, lm_logit  = model_trained(batch_img_tensor, batch_target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_loss)\n",
    "show_img_tensor_CHW(batch_img_tensor[19].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(img_contrastive_prob, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained.text_transformer.text_token_embedding.text_encoder.decode(batch_target_tensor[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs, comments, labels):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    imgs_per_row = 1\n",
    "    fix, axs = plt.subplots(nrows=(len(imgs)+imgs_per_row-1)//imgs_per_row, ncols=imgs_per_row, squeeze=False, figsize=(16,60))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = VF.to_pil_image(img)\n",
    "        row = i // imgs_per_row\n",
    "        col = i % imgs_per_row\n",
    "        axs[row, col].imshow(np.asarray(img))\n",
    "        axs[row, col].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        title = f'pred: {comments[i].replace(\"<pad>\", \"\").replace(\"<bos>\", \"\")}\\nlabel: {labels[i].replace(\"<pad>\", \"\").replace(\"<bos>\", \"\")}'\n",
    "        axs[row, col].set_title(title)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_predicted_commments_index = torch.argmax(img_contrastive_prob, dim=0)\n",
    "\n",
    "show(imgs = [img for img in batch_img_tensor.cpu()], \n",
    "     labels = [model_trained.text_transformer.text_token_embedding.text_encoder.decode(target_tensor) for target_tensor in batch_target_tensor],\n",
    "     comments = [model_trained.text_transformer.text_token_embedding.text_encoder.decode(batch_target_tensor[predicted_comment_index]) for predicted_comment_index in img_predicted_commments_index],\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

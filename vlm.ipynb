{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from config import Config\n",
    "from datetime import datetime\n",
    "from img_embedding import ImageEmbedding\n",
    "from img_transformer import ImgTransformer\n",
    "from img_util import show_img_tensor_CHW\n",
    "from fliker_comment_tokenizer import FlikerCommentTokenizer\n",
    "from fliker_img_comment_dataset import ImgCommentDataset\n",
    "from model_util import count_parameters\n",
    "from pathlib import Path\n",
    "from text_token_embedding import TextTokenEmbedding\n",
    "from text_casual_mask_transformer import TextMaskedTransformer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched img id: /tmp/enriched_results.csv\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "Enriched img id: /tmp/enriched_results.csv\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "Enriched img id: /tmp/enriched_results.csv\n",
      "tokens: 128000\n",
      "tokenizer.is_fast: True\n",
      "train_dataset:  114418\n",
      "eval_dataset:  28605\n",
      "test_dataset:  15892\n",
      "train_dataloader:  15892\n",
      "eval_data_loader:  15892\n",
      "test_data_loader:  15892\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "train_dataset = ImgCommentDataset(config, split=\"train\")\n",
    "eval_dataset = ImgCommentDataset(config, split=\"eval\")\n",
    "test_dataset= ImgCommentDataset(config, split=\"test\")\n",
    "print(f\"train_dataset:  {len(train_dataset)}\")\n",
    "print(f\"eval_dataset:  {len(eval_dataset)}\")\n",
    "print(f\"test_dataset:  {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "# Data Loader\n",
    "BATCH_SIZE = 10\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"train_dataloader:  {len(test_dataset)}\")\n",
    "print(f\"eval_data_loader:  {len(test_dataset)}\")\n",
    "print(f\"test_data_loader:  {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgLanguageModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.img_embedding = ImageEmbedding(config=config)\n",
    "        self.img_transfomer = ImgTransformer(config=config)\n",
    "        self.img_flatten = nn.Flatten(start_dim=1)\n",
    "        self.img_proj = nn.Linear(in_features=config.img_patches* config.img_patch_embedding, out_features=config.img_text_proj_features)\n",
    "        self.img_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # self.text_embedding = TextTokenEmbedding(config=config)\n",
    "        self.text_transformer = TextMaskedTransformer(config=config)\n",
    "        self.text_flatten = nn.Flatten(start_dim=1)\n",
    "        self.text_proj = nn.Linear(in_features=config.max_text_len* config.text_token_embedding, out_features=config.img_text_proj_features)\n",
    "        self.text_softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.diag_mask = torch.diag(torch.ones(config.img_text_proj_features))\n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, batch_img_tensor: torch.tensor, batch_text_tensor: torch.tensor, batch_img_id_tensor: torch.tensor=None):\n",
    "        \"\"\"\n",
    "        batch_img_tensor: B x IMG_PATCHES x IMG_EMB\n",
    "        batch_text_tensor: B x TEXT_TOKEN\n",
    "        \"\"\"\n",
    "        img_embedding = self.img_embedding(batch_img_tensor) # B x IMG_PATCHES x IMG_EMB\n",
    "        # print(f\"img_encoding: {img_embedding.size()}\")\n",
    "\n",
    "        img_feature = self.img_transfomer(img_embedding) # B x IMG_PATCHES x IMG_EMB\n",
    "        # print(f\"img_feature: {img_feature.size()}\")\n",
    "\n",
    "        img_feature = self.img_flatten(img_feature)\n",
    "        # print(f\"img_feature: {img_feature.size()}\")\n",
    "\n",
    "        img_feature = self.img_proj(img_feature)\n",
    "        # print(f\"img_feature: {img_feature.size()}\")  # B x img_text_proj_features\n",
    "\n",
    "        # text_embedding = self.text_embedding(batch_text_tensor)\n",
    "        # print(f\"text_embedding: {text_embedding.size()}\")\n",
    "\n",
    "        text_feature = self.text_transformer(batch_text_tensor)\n",
    "        # print(f\"text_feature: {text_feature.size()}\")\n",
    "\n",
    "        text_feature = self.text_flatten(text_feature)\n",
    "        # print(f\"text_feature: {text_feature.size()}\")\n",
    "\n",
    "        text_feature = self.text_proj(text_feature)\n",
    "        # print(f\"text_feature: {text_feature.size()}\")  # B x img_text_proj_features\n",
    "\n",
    "        # Contrastive learning\n",
    "        contrastive_scores = img_feature @ text_feature.T\n",
    "        # print(f\"contractive_scores: {contrastive_scores}\")  # B x img_text_proj_features\n",
    "\n",
    "        img_contrastive_prob = self.img_softmax(contrastive_scores)\n",
    "        # print(f\"img_contrastive_prob: {img_contrastive_prob}\")  # B x img_text_proj_features\n",
    "        \n",
    "        target = torch.arange(img_contrastive_prob.size()[0], device=img_contrastive_prob.device)\n",
    "        img_loss = self.loss_fn(img_contrastive_prob, target)\n",
    "        # img_loss = self.loss_fn(img_contrastive_prob, self.target.expand(img_contrastive_prob.size()[0], -1))\n",
    "        # print(f\"img_loss: {img_loss}\")\n",
    "\n",
    "        text_contrastive_prob = self.text_softmax(contrastive_scores.T)\n",
    "        # print(f\"text_contrastive_prob: {text_contrastive_prob}\")  # B x img_text_proj_features\n",
    "        text_loss = self.loss_fn(text_contrastive_prob, target)\n",
    "        # print(f\"text_loss: {text_loss}\")\n",
    "        \n",
    "        return img_loss, text_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_img_tensor: torch.Size([10, 3, 512, 512])\n",
      "batch_img_id_tensor: torch.Size([10])\n",
      "batch_comment_encoding: torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = ImgCommentDataset(config, split=\"train\")\n",
    "\n",
    "# BATCH_SIZE = 10\n",
    "# train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "batch_img_tensor, batch_img_id_tensor, batch_comment_encoding = next(iter(train_dataloader))\n",
    "print(f\"batch_img_tensor: {batch_img_tensor.size()}\")\n",
    "print(f\"batch_img_id_tensor: {batch_img_id_tensor.size()}\")\n",
    "print(f\"batch_comment_encoding: {batch_comment_encoding.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImgLanguageModel(config=config)\n",
    "img_loss, text_loss = model(batch_img_tensor=batch_img_tensor, batch_text_tensor=batch_comment_encoding, batch_img_id_tensor=batch_img_id_tensor)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"pytorch_total_params: {pytorch_total_params/10**6} m\")\n",
    "print(f\"pytorch_total_trainable_params: {pytorch_total_trainable_params/10**6} m\")\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 1\n",
    "EVAL_INTERVAL = 100\n",
    "EVAL_STEPS = 10\n",
    "lr = 0.001\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer =  torch.optim.AdamW(params=model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=EPOCHES)\n",
    "\n",
    "def eval(model: ImgLanguageModel, global_step : int, writer: SummaryWriter):\n",
    "    model.eval()\n",
    "\n",
    "    avg_eval_loss = None\n",
    "    eval_loss_std = None\n",
    "    with torch.no_grad():\n",
    "        eval_losses = []\n",
    "        for i, data in enumerate(eval_dataloader):\n",
    "            if i > EVAL_STEPS:\n",
    "                # It takes significant time to do one full eval.\n",
    "                break\n",
    "\n",
    "            batch_img_tensor, batch_target_tensor = data\n",
    "            batch_img_tensor = batch_img_tensor.to(device)\n",
    "            batch_target_tensor = batch_target_tensor.to(device)\n",
    "            img_loss, text_loss = model(batch_img_tensor, batch_target_tensor)\n",
    "            writer.add_scalar(\"eval/Img Loss\", img_loss, global_step)\n",
    "            writer.add_scalar(\"eval/Text Loss\", text_loss, global_step)\n",
    "            eval_losses.append(img_loss + text_loss)\n",
    "        eval_losses = torch.tensor(eval_losses)\n",
    "        avg_eval_loss = eval_losses.mean()\n",
    "        eval_loss_std = eval_losses.std()\n",
    "        writer.add_scalar(\"eval/Loss\", avg_eval_loss, global_step)\n",
    "        writer.add_scalar(\"Loss/eval-std\", eval_loss_std, global_step)\n",
    "    model.train()\n",
    "    writer.flush()\n",
    "    return avg_eval_loss, eval_loss_std\n",
    "    \n",
    "\n",
    "\n",
    "def train(model: ImgLanguageModel, writer: SummaryWriter):\n",
    "    best_vloss = torch.tensor(1_000_000)\n",
    "    with torch.profiler.profile(\n",
    "            schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler('./runs'),\n",
    "            activities=[torch.profiler.ProfilerActivity.CPU], \n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True\n",
    "    ) as prof:\n",
    "    # with torch.mps.profiler.profile(mode=\"interval\", wait_until_completed=False):\n",
    "        for epoch in range(EPOCHES): \n",
    "            for train_step, data in enumerate(train_dataloader):\n",
    "                global_step = epoch * len(train_dataloader) + train_step\n",
    "\n",
    "                # Profile\n",
    "                if global_step < 1 + 1 + 3:\n",
    "                    prof.step()\n",
    "\n",
    "                batch_img_tensor, batch_img_id_tensor, batch_target_tensor = data\n",
    "                batch_img_tensor = batch_img_tensor.to(device)\n",
    "                batch_target_tensor = batch_target_tensor.to(device)\n",
    "\n",
    "                # Viz Model\n",
    "                if global_step == 0:\n",
    "                    writer.add_graph(model, (batch_img_tensor, batch_target_tensor))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                img_loss, text_loss = model(batch_img_tensor, batch_target_tensor)\n",
    "                writer.add_scalar(\"train/Img Loss\", img_loss, global_step)\n",
    "                writer.add_scalar(\"train/Text Loss\", text_loss, global_step)\n",
    "                writer.add_scalar(\"train/Loss\", img_loss+text_loss, global_step)\n",
    "                writer.add_scalar(\"Learning Rate\", scheduler.get_last_lr()[-1], global_step)\n",
    "                loss = img_loss + text_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                if train_step > 0 and train_step % EVAL_INTERVAL == 0:\n",
    "                    avg_vloss, _ = eval(model=model, global_step=global_step, writer=writer)\n",
    "                \n",
    "                    if avg_vloss is not None and avg_vloss < best_vloss:\n",
    "                        best_vloss = avg_vloss\n",
    "                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                        model_path = f\"multi_label_img_classifier_{epoch}_{timestamp}\"\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "with SummaryWriter(flush_secs=1) as writer:\n",
    "    train(model=model, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
